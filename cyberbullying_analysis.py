# -*- coding: utf-8 -*-
"""Cyberbullying_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BszAYv2KokQPtfHI5fF-gpjN3dHn_MLP

U.Vasanth | 2348463
"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from collections import Counter
import string

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

def preprocess_text(text):
    # Tokenize the text
    sentences = sent_tokenize(text)

    # Remove stopwords and punctuation, but keep some common words
    stop_words = set(stopwords.words('english')) - {'not', 'no', 'but'}
    clean_sentences = []
    for sentence in sentences:
        words = word_tokenize(sentence.lower())
        words = [word for word in words if word not in string.punctuation and (word not in stop_words or len(word) > 1)]
        clean_sentences.append(' '.join(words))

    return clean_sentences

def summarize_text(text, num_sentences=3):
    # Preprocess the text
    sentences = preprocess_text(text)

    # If we have fewer sentences than requested, return all sentences
    if len(sentences) <= num_sentences:
        return ' '.join(sentences)

    # Create TF-IDF matrix
    vectorizer = TfidfVectorizer(min_df=1)  # Allow terms that appear in at least 1 document
    tfidf_matrix = vectorizer.fit_transform(sentences)

    # Calculate sentence scores
    sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)

    # Get top sentences
    top_sentence_indices = sentence_scores.argsort()[-num_sentences:][::-1]

    # Return summary
    summary = [sent_tokenize(text)[i] for i in sorted(top_sentence_indices)]
    return ' '.join(summary)

def analyze_cyberbullying(file_path):
    # Load the dataset
    df = pd.read_csv(file_path)

    # Summarize each cyberbullying instance
    df['summary'] = df['tweet_text'].apply(lambda x: summarize_text(x) if isinstance(x, str) else '')

    # Analyze cyberbullying types
    cyberbullying_types = df['cyberbullying_type'].value_counts()

    # Identify common words in cyberbullying tweets
    all_words = ' '.join(df['tweet_text'].dropna()).lower().split()
    word_freq = Counter(all_words)
    common_words = word_freq.most_common(10)

    return df, cyberbullying_types, common_words

# Main execution
if __name__ == "__main__":
    file_path = "/content/cyberbullying_tweets.csv"
    df, cyberbullying_types, common_words = analyze_cyberbullying(file_path)

    print("Sample of summarized cyberbullying tweets:")
    print(df[['tweet_text', 'summary', 'cyberbullying_type']].head())

    print("\nCyberbullying types distribution:")
    print(cyberbullying_types)

    print("\nMost common words in cyberbullying tweets:")
    print(common_words)

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import string

nltk.download('punkt')
nltk.download('stopwords')

def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text.lower())
    return [word for word in words if word.isalnum() and word not in stop_words and len(word) > 2]

def analyze_cyberbullying(file_path):
    df = pd.read_csv(file_path)

    # Analyze cyberbullying types
    cyberbullying_types = df['cyberbullying_type'].value_counts()

    # Identify common words in cyberbullying tweets (excluding 'not_cyberbullying')
    cyberbullying_tweets = df[df['cyberbullying_type'] != 'not_cyberbullying']['tweet_text']
    all_words = [word for tweet in cyberbullying_tweets for word in preprocess_text(tweet)]
    word_freq = Counter(all_words)
    common_words = word_freq.most_common(20)

    # Get examples of each cyberbullying type
    examples = {}
    for bullying_type in df['cyberbullying_type'].unique():
        examples[bullying_type] = df[df['cyberbullying_type'] == bullying_type]['tweet_text'].head(2).tolist()

    return cyberbullying_types, common_words, examples

# Main execution
if __name__ == "__main__":
    file_path = "/content/cyberbullying_tweets.csv"
    cyberbullying_types, common_words, examples = analyze_cyberbullying(file_path)

    print("Cyberbullying types distribution:")
    print(cyberbullying_types)

    print("\nMost common words in cyberbullying tweets:")
    print(common_words)

    print("\nExamples of each cyberbullying type:")
    for bullying_type, tweets in examples.items():
        print(f"\n{bullying_type.upper()}:")
        for tweet in tweets:
            print(f"- {tweet}")